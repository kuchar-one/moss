{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽµ MOSS: Multi-Objective Sound Synthesis\n",
    "\n",
    "## Pareto Optimization for Audio-Visual Spectrogram Blending\n",
    "\n",
    "This notebook demonstrates the core algorithms behind MOSS - a system that creates audio spectrograms\n",
    "that simultaneously look like a target image and sound like a target audio track.\n",
    "\n",
    "**The Problem**: Given:\n",
    "- A target **image** (e.g., Mona Lisa)\n",
    "- A target **audio** (e.g., Tchaikovsky)\n",
    "\n",
    "Find a spectrogram that:\n",
    "1. **Looks like** the image (when visualized)\n",
    "2. **Sounds like** the audio (when converted back to waveform)\n",
    "\n",
    "This is inherently a **multi-objective optimization** problem with two conflicting goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from PIL import Image\n",
    "from pytorch_msssim import ssim\n",
    "from IPython.display import HTML, Audio, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_RATE = 16000  # 16kHz for efficiency\n",
    "N_FFT = 1024         # 513 frequency bins\n",
    "HOP_LENGTH = 256     # 75% overlap\n",
    "WIN_LENGTH = N_FFT\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Algorithm: Mask-Based Spectrogram Blending\n",
    "\n",
    "The key insight is that we can **blend** two spectrograms using a learnable mask:\n",
    "\n",
    "$$\\text{Mixed} = \\text{Mask} \\cdot \\text{Image} + (1 - \\text{Mask}) \\cdot \\text{Audio}$$\n",
    "\n",
    "Where:\n",
    "- **Mask** âˆˆ [0, 1] for each time-frequency bin\n",
    "- **Mask = 1**: Show the image (visual priority)\n",
    "- **Mask = 0**: Keep the audio (audio priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_blur_2d(x, sigma=1.0):\n",
    "    \"\"\"Apply Gaussian smoothing to enforce musicality constraints.\"\"\"\n",
    "    if sigma < 0.5:\n",
    "        return x\n",
    "    \n",
    "    kernel_size = int(6 * sigma) | 1  # Ensure odd\n",
    "    kernel_size = max(3, kernel_size)\n",
    "    \n",
    "    # Create 1D Gaussian kernel\n",
    "    coords = torch.arange(kernel_size, dtype=x.dtype, device=x.device) - kernel_size // 2\n",
    "    g = torch.exp(-coords**2 / (2 * sigma**2))\n",
    "    g = g / g.sum()\n",
    "    \n",
    "    # Separable 2D convolution\n",
    "    pad = kernel_size // 2\n",
    "    if x.dim() == 2:\n",
    "        x = x.unsqueeze(0).unsqueeze(0)\n",
    "    elif x.dim() == 3:\n",
    "        x = x.unsqueeze(1)\n",
    "    \n",
    "    x = F.conv2d(F.pad(x, (pad, pad, 0, 0), mode='replicate'), \n",
    "                 g.view(1, 1, 1, -1), padding=0)\n",
    "    x = F.conv2d(F.pad(x, (0, 0, pad, pad), mode='replicate'), \n",
    "                 g.view(1, 1, -1, 1), padding=0)\n",
    "    \n",
    "    return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss Functions\n",
    "\n",
    "### Visual Loss: SSIM (Structural Similarity)\n",
    "We compare the **visual appearance** of the mixed spectrogram to the target image using SSIM,\n",
    "which captures structural patterns better than pixel-wise L1/L2.\n",
    "\n",
    "### Audio Loss: Log-Domain L1\n",
    "We compare audio in the **log-magnitude domain** (dB-like), which matches human perception\n",
    "where a 10x difference in power = 10dB = perceptually \"twice as loud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_image_loss(mixed_mag, target_image_01):\n",
    "    \"\"\"SSIM-based visual similarity in log domain.\"\"\"\n",
    "    mixed_log = torch.log(mixed_mag + 1e-8)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    flat = mixed_log.flatten(start_dim=-2)\n",
    "    min_val = flat.min(dim=-1, keepdim=True)[0].unsqueeze(-1)\n",
    "    max_val = flat.max(dim=-1, keepdim=True)[0].unsqueeze(-1)\n",
    "    mixed_norm = (mixed_log - min_val) / (max_val - min_val + 1e-8)\n",
    "    \n",
    "    target_norm = target_image_01.expand_as(mixed_norm)\n",
    "    \n",
    "    # Add channel dim for SSIM\n",
    "    mixed_norm = mixed_norm.unsqueeze(1)\n",
    "    target_norm = target_norm.unsqueeze(1)\n",
    "    \n",
    "    ssim_vals = []\n",
    "    for i in range(mixed_mag.shape[0]):\n",
    "        s = ssim(mixed_norm[i:i+1], target_norm[i:i+1], data_range=1.0, size_average=True)\n",
    "        ssim_vals.append(1.0 - s)  # Loss = 1 - SSIM\n",
    "    \n",
    "    return torch.stack(ssim_vals)\n",
    "\n",
    "\n",
    "def calc_audio_loss(mixed_mag, target_audio_mag):\n",
    "    \"\"\"L1 loss in log-magnitude domain (perceptually weighted).\"\"\"\n",
    "    mixed_log = torch.log(mixed_mag + 1e-8)\n",
    "    target_log = torch.log(target_audio_mag + 1e-8)\n",
    "    target_log = target_log.expand_as(mixed_log)\n",
    "    \n",
    "    loss = F.l1_loss(mixed_log, target_log, reduction='none').mean(dim=(1, 2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Encoder: Converting Mask to Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskEncoder(nn.Module):\n",
    "    \"\"\"Encodes a mask grid into a blended spectrogram and reconstructed audio.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_image, audio_path, grid_h=64, grid_w=128, sigma=5.0):\n",
    "        super().__init__()\n",
    "        self.grid_height = grid_h\n",
    "        self.grid_width = grid_w\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # Load audio\n",
    "        audio, sr = torchaudio.load(audio_path)\n",
    "        if sr != SAMPLE_RATE:\n",
    "            audio = torchaudio.functional.resample(audio, sr, SAMPLE_RATE)\n",
    "        audio = audio.mean(dim=0, keepdim=True)  # Mono\n",
    "        self.register_buffer('audio_waveform', audio)\n",
    "        \n",
    "        # Compute STFT\n",
    "        window = torch.hann_window(N_FFT)\n",
    "        stft = torch.stft(audio, n_fft=N_FFT, hop_length=HOP_LENGTH, \n",
    "                          win_length=WIN_LENGTH, window=window, return_complex=True)\n",
    "        \n",
    "        self.audio_mag = stft.abs() + 1e-8\n",
    "        self.audio_phase = stft.angle()\n",
    "        self.full_h, self.full_w = self.audio_mag.shape[1], self.audio_mag.shape[2]\n",
    "        \n",
    "        # Process image\n",
    "        img = target_image.unsqueeze(0) if target_image.dim() == 3 else target_image\n",
    "        img = F.interpolate(img, size=(self.full_h, self.full_w), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Map image to audio dynamic range\n",
    "        audio_log = torch.log(self.audio_mag)\n",
    "        audio_log_max = torch.quantile(audio_log, 0.98)\n",
    "        audio_log_floor = torch.quantile(audio_log, 0.01)\n",
    "        \n",
    "        img_01 = img.squeeze(0)\n",
    "        img_01 = (img_01 - img_01.min()) / (img_01.max() - img_01.min() + 1e-8)\n",
    "        img_01 = img_01.pow(1.8)  # Gamma correction\n",
    "        \n",
    "        image_log = img_01 * (audio_log_max - audio_log_floor) + audio_log_floor\n",
    "        self.image_mag = torch.exp(image_log)\n",
    "        \n",
    "        # Reference for loss calculation\n",
    "        self.image_mag_ref = self.image_mag\n",
    "        self.audio_mag_ref = self.audio_mag\n",
    "        \n",
    "    def forward(self, mask_flat, return_wav=True):\n",
    "        \"\"\"Forward pass: mask -> blended spectrogram -> audio.\"\"\"\n",
    "        B = mask_flat.shape[0]\n",
    "        \n",
    "        # Reshape mask\n",
    "        mask = mask_flat.view(B, self.grid_height, self.grid_width)\n",
    "        \n",
    "        # Upsample to full resolution\n",
    "        mask_up = F.interpolate(mask.unsqueeze(1), size=(self.full_h, self.full_w),\n",
    "                                mode='bilinear', align_corners=False).squeeze(1)\n",
    "        \n",
    "        # Apply Gaussian smoothing (forces musical smoothness)\n",
    "        mask_smooth = gaussian_blur_2d(mask_up, self.sigma)\n",
    "        mask_smooth = mask_smooth.clamp(0, 1)\n",
    "        \n",
    "        # Blend spectrograms\n",
    "        mixed_mag = mask_smooth * self.image_mag + (1 - mask_smooth) * self.audio_mag\n",
    "        \n",
    "        if not return_wav:\n",
    "            return None, mixed_mag\n",
    "        \n",
    "        # Reconstruct audio via ISTFT\n",
    "        mixed_complex = mixed_mag * torch.exp(1j * self.audio_phase)\n",
    "        window = torch.hann_window(N_FFT, device=mixed_mag.device)\n",
    "        \n",
    "        wavs = []\n",
    "        for i in range(B):\n",
    "            wav = torch.istft(mixed_complex[i], n_fft=N_FFT, hop_length=HOP_LENGTH,\n",
    "                              win_length=WIN_LENGTH, window=window)\n",
    "            wavs.append(wav)\n",
    "        \n",
    "        return torch.stack(wavs), mixed_mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data: Mona Lisa + Tchaikovsky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img_path = 'data/input/monalisa.jpg'\n",
    "img = Image.open(img_path).convert('L')  # Grayscale\n",
    "img_tensor = torch.from_numpy(np.array(img)).float() / 255.0\n",
    "img_tensor = img_tensor.unsqueeze(0)  # Add channel dim\n",
    "\n",
    "# Audio path\n",
    "audio_path = 'data/input/tchaikovsky.mp3'\n",
    "\n",
    "# Create encoder\n",
    "encoder = MaskEncoder(img_tensor, audio_path, grid_h=64, grid_w=128, sigma=5.0)\n",
    "\n",
    "print(f\"Spectrogram shape: {encoder.full_h} x {encoder.full_w}\")\n",
    "print(f\"Control grid: {encoder.grid_height} x {encoder.grid_width}\")\n",
    "print(f\"Total parameters: {encoder.grid_height * encoder.grid_width}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize inputs\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Target image\n",
    "axes[0].imshow(img, cmap='gray')\n",
    "axes[0].set_title('Target Image: Mona Lisa')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Image as spectrogram\n",
    "axes[1].imshow(20*torch.log10(encoder.image_mag[0]+1e-8).numpy(), \n",
    "               origin='lower', aspect='auto', cmap='magma')\n",
    "axes[1].set_title('Image â†’ Spectrogram')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Audio spectrogram\n",
    "axes[2].imshow(20*torch.log10(encoder.audio_mag[0]+1e-8).numpy(), \n",
    "               origin='lower', aspect='auto', cmap='magma')\n",
    "axes[2].set_title('Audio: Tchaikovsky')\n",
    "axes[2].set_xlabel('Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Single-Objective Optimization\n",
    "\n",
    "First, let's optimize for a **single weighted combination** of the two objectives:\n",
    "\n",
    "$$\\mathcal{L}_{total} = w_{vis} \\cdot \\mathcal{L}_{visual} + w_{aud} \\cdot \\mathcal{L}_{audio}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_objective_optimize(encoder, w_vis=0.5, w_aud=0.5, steps=200, lr=0.1):\n",
    "    \"\"\"Optimize a single mask for a weighted combination of objectives.\"\"\"\n",
    "    \n",
    "    # Initialize mask in logit space (unbounded, sigmoid to [0,1])\n",
    "    mask_logits = nn.Parameter(torch.zeros(1, encoder.grid_height * encoder.grid_width))\n",
    "    optimizer = optim.Adam([mask_logits], lr=lr)\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mask = torch.sigmoid(mask_logits)\n",
    "        _, mixed_mag = encoder(mask, return_wav=False)\n",
    "        \n",
    "        loss_vis = calc_image_loss(mixed_mag, encoder.image_mag_ref).mean()\n",
    "        loss_aud = calc_audio_loss(mixed_mag, encoder.audio_mag_ref).mean()\n",
    "        \n",
    "        total_loss = w_vis * loss_vis + w_aud * loss_aud\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        history.append([loss_vis.item(), loss_aud.item()])\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: Vis={loss_vis.item():.4f}, Aud={loss_aud.item():.4f}\")\n",
    "    \n",
    "    return torch.sigmoid(mask_logits).detach(), np.array(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run three different weightings\n",
    "print(\"\\nðŸ“Š Optimizing: Visual Priority (80/20)\")\n",
    "mask_visual, hist_visual = single_objective_optimize(encoder, w_vis=0.8, w_aud=0.2, steps=200)\n",
    "\n",
    "print(\"\\nðŸ“Š Optimizing: Balanced (50/50)\")\n",
    "mask_balanced, hist_balanced = single_objective_optimize(encoder, w_vis=0.5, w_aud=0.5, steps=200)\n",
    "\n",
    "print(\"\\nðŸ“Š Optimizing: Audio Priority (20/80)\")\n",
    "mask_audio, hist_audio = single_objective_optimize(encoder, w_vis=0.2, w_aud=0.8, steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "masks = [mask_visual, mask_balanced, mask_audio]\n",
    "titles = ['Visual Priority', 'Balanced', 'Audio Priority']\n",
    "\n",
    "for i, (mask, title) in enumerate(zip(masks, titles)):\n",
    "    _, mixed_mag = encoder(mask, return_wav=False)\n",
    "    \n",
    "    # Show mask\n",
    "    axes[0, i].imshow(mask.view(64, 128).numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0, i].set_title(f'{title}\\nMask')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Show spectrogram\n",
    "    axes[1, i].imshow(20*torch.log10(mixed_mag[0]+1e-8).numpy(),\n",
    "                      origin='lower', aspect='auto', cmap='magma')\n",
    "    axes[1, i].set_title('Result Spectrogram')\n",
    "    axes[1, i].set_xlabel('Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to results\n",
    "print(\"ðŸ”Š Audio Priority Result:\")\n",
    "wav_audio, _ = encoder(mask_audio, return_wav=True)\n",
    "display(Audio(wav_audio[0].numpy(), rate=SAMPLE_RATE))\n",
    "\n",
    "print(\"\\nðŸ”Š Balanced Result:\")\n",
    "wav_balanced, _ = encoder(mask_balanced, return_wav=True)\n",
    "display(Audio(wav_balanced[0].numpy(), rate=SAMPLE_RATE))\n",
    "\n",
    "print(\"\\nðŸ”Š Visual Priority Result:\")\n",
    "wav_visual, _ = encoder(mask_visual, return_wav=True)\n",
    "display(Audio(wav_visual[0].numpy(), rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Pareto Frontier: Multi-Objective Optimization\n",
    "\n",
    "Instead of choosing a single weighting, we can find the **entire Pareto frontier** - \n",
    "all solutions where you cannot improve one objective without worsening the other.\n",
    "\n",
    "### Our Hybrid Approach:\n",
    "1. **Gradient Seeding** (200 steps): Run Adam with different weight combinations in parallel\n",
    "2. **Evolutionary Expansion** (NSGA-II): Use genetic operators to explore the frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_seeding(encoder, pop_size=10, steps=200, lr=0.05):\n",
    "    \"\"\"\n",
    "    Phase 1: Generate diverse seeds using parallel gradient descent.\n",
    "    Uses power-law weight distribution to counteract visual loss dominance.\n",
    "    \"\"\"\n",
    "    # Initialize population in logit space\n",
    "    n_params = encoder.grid_height * encoder.grid_width\n",
    "    mask_logits = nn.Parameter(torch.randn(pop_size, n_params) * 0.5)\n",
    "    \n",
    "    # Power-law weights to balance visual (20x boosted) vs audio\n",
    "    alpha = torch.linspace(0, 1, pop_size)\n",
    "    weights_vis = (1.0 - alpha).pow(4.0)  # Bunch towards low visual weight\n",
    "    weights_aud = 1.0 - weights_vis\n",
    "    total = weights_vis + weights_aud\n",
    "    weights_vis /= total\n",
    "    weights_aud /= total\n",
    "    \n",
    "    # Force anchors\n",
    "    with torch.no_grad():\n",
    "        mask_logits[0].fill_(10.0)   # Pure image\n",
    "        mask_logits[-1].fill_(-10.0) # Pure audio\n",
    "    \n",
    "    optimizer = optim.Adam([mask_logits], lr=lr)\n",
    "    history = []\n",
    "    \n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        masks = torch.sigmoid(mask_logits)\n",
    "        _, mixed_mag = encoder(masks, return_wav=False)\n",
    "        \n",
    "        # Calculate losses\n",
    "        diff = torch.abs(mixed_mag - encoder.image_mag_ref)\n",
    "        loss_vis = diff.mean(dim=(1, 2))\n",
    "        loss_aud = calc_audio_loss(mixed_mag, encoder.audio_mag_ref)\n",
    "        \n",
    "        # Weighted sum (20x boost on visual for gradient direction)\n",
    "        total_loss = (weights_vis * loss_vis * 20.0 + weights_aud * loss_aud).sum()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Force anchor masks\n",
    "        with torch.no_grad():\n",
    "            mask_logits[0].fill_(20.0)\n",
    "            mask_logits[-1].fill_(-20.0)\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            history.append(np.column_stack([loss_vis.detach().numpy(), \n",
    "                                           loss_aud.detach().numpy()]))\n",
    "    \n",
    "    return torch.sigmoid(mask_logits).detach().numpy(), history\n",
    "\n",
    "print(\"ðŸŒ± Running Gradient Seeding...\")\n",
    "seed_masks, seed_history = gradient_seeding(encoder, pop_size=10, steps=200)\n",
    "print(f\"Generated {len(seed_masks)} seed solutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize seeding progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, frame in enumerate(seed_history):\n",
    "    alpha = 0.3 + 0.7 * (i / len(seed_history))\n",
    "    plt.scatter(frame[:, 0], frame[:, 1], c='purple', alpha=alpha, s=40)\n",
    "\n",
    "plt.xlabel('Visual Loss')\n",
    "plt.ylabel('Audio Loss')\n",
    "plt.title('Phase 1: Gradient Seeding Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Evolutionary Expansion (NSGA-II)\n",
    "\n",
    "NSGA-II is a state-of-the-art multi-objective genetic algorithm that:\n",
    "1. Uses **non-dominated sorting** to rank solutions\n",
    "2. Maintains **diversity** via crowding distance\n",
    "3. Applies **crossover** (SBX) and **mutation** (PM) operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.core.problem import Problem\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.operators.crossover.sbx import SBX\n",
    "from pymoo.operators.mutation.pm import PM\n",
    "from pymoo.core.callback import Callback\n",
    "\n",
    "\n",
    "class MOSSProblem(Problem):\n",
    "    \"\"\"Pymoo problem definition for MOSS.\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder):\n",
    "        self.encoder = encoder\n",
    "        n_params = encoder.grid_height * encoder.grid_width\n",
    "        super().__init__(n_var=n_params, n_obj=2, xl=0.0, xu=1.0)\n",
    "    \n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            mask = torch.from_numpy(x).float()\n",
    "            _, mixed_mag = self.encoder(mask, return_wav=False)\n",
    "            \n",
    "            diff = torch.abs(mixed_mag - self.encoder.image_mag_ref)\n",
    "            loss_vis = diff.mean(dim=(1, 2)).numpy()\n",
    "            loss_aud = calc_audio_loss(mixed_mag, self.encoder.audio_mag_ref).numpy()\n",
    "            \n",
    "            out['F'] = np.column_stack([loss_vis, loss_aud])\n",
    "\n",
    "\n",
    "class HistoryCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.history = []\n",
    "    \n",
    "    def notify(self, algorithm):\n",
    "        self.history.append(algorithm.pop.get('F').copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NSGA-II\n",
    "print(\"ï¿½ï¿½ Running NSGA-II Evolution...\")\n",
    "\n",
    "problem = MOSSProblem(encoder)\n",
    "\n",
    "# Initialize with seeds + random\n",
    "n_random = 100 - len(seed_masks)\n",
    "X_init = np.vstack([seed_masks, np.random.rand(n_random, problem.n_var)])\n",
    "\n",
    "algorithm = NSGA2(\n",
    "    pop_size=100,\n",
    "    sampling=X_init,\n",
    "    crossover=SBX(eta=15, prob=0.9),\n",
    "    mutation=PM(eta=20),\n",
    "    eliminate_duplicates=True\n",
    ")\n",
    "\n",
    "callback = HistoryCallback()\n",
    "\n",
    "result = minimize(\n",
    "    problem,\n",
    "    algorithm,\n",
    "    ('n_gen', 50),\n",
    "    callback=callback,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Found {len(result.F)} Pareto-optimal solutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final Pareto front\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Final front\n",
    "plt.scatter(result.F[:, 0], result.F[:, 1], c='#4ade80', s=80, \n",
    "            edgecolors='white', linewidths=1, label='Pareto Front', zorder=10)\n",
    "\n",
    "# Seeds for reference\n",
    "seed_F = seed_history[-1] if seed_history else None\n",
    "if seed_F is not None:\n",
    "    plt.scatter(seed_F[:, 0], seed_F[:, 1], c='purple', s=60, alpha=0.5, label='Initial Seeds')\n",
    "\n",
    "plt.xlabel('Visual Loss â†“')\n",
    "plt.ylabel('Audio Loss â†“')\n",
    "plt.title('Pareto Frontier: Visual vs Audio Quality')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Animated Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animation\n",
    "fig, ax = plt.subplots(figsize=(10, 6), facecolor='#09090b')\n",
    "ax.set_facecolor('#09090b')\n",
    "ax.set_title('Hybrid Pareto Optimization', color='white', fontsize=14)\n",
    "ax.set_xlabel('Visual Loss', color='#666')\n",
    "ax.set_ylabel('Audio Loss', color='#666')\n",
    "ax.tick_params(colors='#444')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color('#333')\n",
    "ax.grid(True, alpha=0.15, color='#444')\n",
    "\n",
    "# Compute bounds\n",
    "all_F = np.vstack(seed_history + callback.history)\n",
    "ax.set_xlim(all_F[:, 0].min() * 0.9, all_F[:, 0].max() * 1.1)\n",
    "ax.set_ylim(all_F[:, 1].min() * 0.9, all_F[:, 1].max() * 1.1)\n",
    "\n",
    "scat = ax.scatter([], [], c='#a855f7', s=60, edgecolors='white', linewidths=0.5)\n",
    "text = ax.text(0.02, 0.98, '', transform=ax.transAxes, color='white', fontsize=10, va='top')\n",
    "\n",
    "full_history = seed_history + callback.history\n",
    "phase1_len = len(seed_history)\n",
    "\n",
    "def update(frame):\n",
    "    if frame >= len(full_history):\n",
    "        return scat, text\n",
    "    \n",
    "    data = full_history[frame]\n",
    "    scat.set_offsets(data)\n",
    "    \n",
    "    if frame < phase1_len:\n",
    "        scat.set_facecolors('#a855f7')\n",
    "        text.set_text(f'Phase 1: Gradient Seeding (Step {frame * 10})')\n",
    "    else:\n",
    "        scat.set_facecolors('#4ade80')\n",
    "        text.set_text(f'Phase 2: Evolution (Gen {frame - phase1_len})')\n",
    "    \n",
    "    return scat, text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=len(full_history), interval=100, blit=True)\n",
    "plt.close()\n",
    "\n",
    "# Display in notebook\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Explore the Pareto Front\n",
    "\n",
    "Each point on the front represents a different trade-off. Let's listen to a few!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by visual loss\n",
    "sorted_idx = np.argsort(result.F[:, 0])\n",
    "\n",
    "# Pick samples from different parts of the front\n",
    "samples = [sorted_idx[0], sorted_idx[len(sorted_idx)//2], sorted_idx[-1]]\n",
    "labels = ['Best Visual', 'Balanced', 'Best Audio']\n",
    "\n",
    "for idx, label in zip(samples, labels):\n",
    "    mask = torch.from_numpy(result.X[idx:idx+1]).float()\n",
    "    wav, mixed_mag = encoder(mask, return_wav=True)\n",
    "    \n",
    "    print(f\"\\nðŸŽµ {label} (Vis: {result.F[idx, 0]:.4f}, Aud: {result.F[idx, 1]:.4f})\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "    axes[0].imshow(mask.view(64, 128).numpy(), cmap='gray')\n",
    "    axes[0].set_title('Mask')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(20*torch.log10(mixed_mag[0]+1e-8).numpy(),\n",
    "                   origin='lower', aspect='auto', cmap='magma')\n",
    "    axes[1].set_title('Spectrogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    display(Audio(wav[0].numpy(), rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Design Decisions:\n",
    "\n",
    "1. **Mask-Based Blending**: Instead of generating spectrograms from scratch, we blend\n",
    "   existing image and audio spectrograms using a learnable mask. This preserves audio quality.\n",
    "\n",
    "2. **Gaussian Smoothing**: Forces the mask to vary slowly in time-frequency, creating\n",
    "   smoother, more musical transitions instead of harsh noise.\n",
    "\n",
    "3. **SSIM for Visual Loss**: Captures structural patterns, not just pixel differences.\n",
    "   A blurry but structurally correct image scores better than a sharp but wrong one.\n",
    "\n",
    "4. **Log-Domain Audio Loss**: Matches human perception where equal dB differences feel\n",
    "   equally significant regardless of absolute level.\n",
    "\n",
    "5. **Hybrid Optimization**: Gradient descent finds initial seeds efficiently,\n",
    "   while NSGA-II explores the trade-off space more thoroughly.\n",
    "\n",
    "6. **Power-Law Weight Distribution**: Counteracts the natural dominance of visual loss\n",
    "   to ensure balanced exploration of the Pareto front.\n",
    "\n",
    "---\n",
    "*Made with ðŸ’œ by VojtÄ›ch KuchaÅ™*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
