{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéµ MOSS: Multi-Objective Sound Synthesis\n",
    "\n",
    "## Pareto Optimization for Audio-Visual Spectrogram Blending\n",
    "\n",
    "This notebook is a **self-contained, comprehensive demonstration** of the MOSS system. It implements\n",
    "the **exact same algorithms** as the web frontend, allowing you to understand and experiment with\n",
    "the optimization process.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Given:\n",
    "- A target **image** (e.g., Mona Lisa)\n",
    "- A target **audio** (e.g., Tchaikovsky)\n",
    "\n",
    "Find a spectrogram that:\n",
    "1. **Looks like** the image when visualized\n",
    "2. **Sounds like** the audio when converted back to waveform\n",
    "\n",
    "This is inherently a **multi-objective optimization** problem because improving visual fidelity\n",
    "typically degrades audio quality, and vice versa.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Mask-Based Blending**: We blend image and audio spectrograms using a learnable mask\n",
    "2. **Pareto Optimality**: A solution is Pareto-optimal if you can't improve one objective without worsening the other\n",
    "3. **Hybrid Optimization**: We combine gradient descent (for fast seeding) with evolutionary algorithms (for exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Imports\n",
    "\n",
    "All parameters match the production MOSS system exactly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')  # Add project root to path\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from PIL import Image\n",
    "from IPython.display import HTML, Audio, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Copied from src/config.py EXACTLY\n",
    "# ============================================================================\n",
    "SAMPLE_RATE = 16000       # 16kHz for efficiency\n",
    "N_FFT = 1024              # 513 frequency bins (N_FFT/2 + 1)\n",
    "HOP_LENGTH = 256          # 75% overlap between frames\n",
    "WIN_LENGTH = N_FFT\n",
    "\n",
    "# Control grid resolution (the optimization variable)\n",
    "GRID_HEIGHT = 64\n",
    "GRID_WIDTH = 128\n",
    "N_PARAMS = GRID_HEIGHT * GRID_WIDTH  # 8192 parameters\n",
    "\n",
    "# Proxy optimization for speed (1/4 resolution)\n",
    "PROXY_HEIGHT = 129        # 513 // 4 ‚âà 128, use 129 for safety\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "# Optimization hyperparameters (EXACT match to production)\n",
    "SINGLE_STEPS = 500        # Steps for single-objective optimization\n",
    "PARETO_SEED_STEPS = 200   # Gradient seeding steps for Pareto\n",
    "PARETO_GENERATIONS = 50   # NSGA-II generations for Pareto\n",
    "PARETO_SEED_POP = 10      # Population for gradient seeding\n",
    "PARETO_EVOL_POP = 100     # Population for evolutionary phase\n",
    "\n",
    "def plot_spectrogram(ax, mag_tensor, title=None, show_colorbar=False):\n",
    "    \"\"\"\n",
    "    Plot spectrogram with EXACT same scaling as frontend.\n",
    "    \n",
    "    Uses robust percentile-based scaling:\n",
    "    - vmax = 99.5th percentile of dB values\n",
    "    - vmin = vmax - 80 dB (80 dB dynamic range)\n",
    "    \"\"\"\n",
    "    spec_db = 20 * torch.log10(mag_tensor + 1e-8).cpu().numpy()\n",
    "    ref_max = np.percentile(spec_db, 99.5)\n",
    "    vmin = ref_max - 80\n",
    "    vmax = ref_max\n",
    "    \n",
    "    im = ax.imshow(spec_db, origin='lower', aspect='auto', cmap='magma',\n",
    "                   vmin=vmin, vmax=vmax)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    if show_colorbar:\n",
    "        plt.colorbar(im, ax=ax, label='dB')\n",
    "    return im\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Control grid: {GRID_HEIGHT}√ó{GRID_WIDTH} = {N_PARAMS} parameters\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss Functions\n",
    "\n",
    "### Visual Loss: Mean Absolute Error (MAE)\n",
    "\n",
    "$$\\mathcal{L}_{visual} = \\frac{1}{F \\cdot T} \\sum_{f,t} |M_{mixed}(f,t) - M_{image}(f,t)|$$\n",
    "\n",
    "We use MAE in linear magnitude domain for computational efficiency.\n",
    "\n",
    "### Audio Loss: Log-Domain L1\n",
    "\n",
    "$$\\mathcal{L}_{audio} = \\frac{1}{F \\cdot T} \\sum_{f,t} |\\log(M_{mixed}(f,t)) - \\log(M_{audio}(f,t))|$$\n",
    "\n",
    "The log domain matches human perception: equal dB differences feel equally significant.\n",
    "\n",
    "### Loss Normalization\n",
    "\n",
    "Both losses are normalized to [0, 1] by dividing by their worst-case values:\n",
    "- **Worst visual**: Pure audio spectrogram (mask = 0)\n",
    "- **Worst audio**: Pure image spectrogram (mask = 1)\n",
    "\n",
    "### 20√ó Visual Boost\n",
    "\n",
    "During optimization, visual loss is multiplied by 20√ó to ensure the image is clearly visible.\n",
    "This compensates for the natural dominance of audio preservation in the loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calc_audio_mag_loss(mixed_mag, target_audio_mag):\n",
    "    \"\"\"\n",
    "    Audio loss: L1 in log-magnitude domain.\n",
    "    \n",
    "    This matches human perception where equal dB differences\n",
    "    feel equally significant regardless of absolute level.\n",
    "    \n",
    "    Args:\n",
    "        mixed_mag: (B, F, T) or (F, T) - mixed spectrogram magnitude\n",
    "        target_audio_mag: (1, F, T) or (F, T) - target audio magnitude\n",
    "    \n",
    "    Returns:\n",
    "        Loss tensor of shape (B,) or scalar\n",
    "    \"\"\"\n",
    "    mixed_log = torch.log(mixed_mag + 1e-8)\n",
    "    target_log = torch.log(target_audio_mag + 1e-8)\n",
    "    target_log = target_log.expand_as(mixed_log)\n",
    "    \n",
    "    loss = F.l1_loss(mixed_log, target_log, reduction='none')\n",
    "    \n",
    "    if mixed_mag.dim() == 3:\n",
    "        return loss.mean(dim=(1, 2))  # (B,)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def calc_visual_loss(mixed_mag, target_image_mag):\n",
    "    \"\"\"\n",
    "    Visual loss: Mean Absolute Error in linear magnitude domain.\n",
    "    \n",
    "    Args:\n",
    "        mixed_mag: (B, F, T) or (F, T)\n",
    "        target_image_mag: (1, F, T) or (F, T)\n",
    "    \n",
    "    Returns:\n",
    "        Loss tensor of shape (B,) or scalar\n",
    "    \"\"\"\n",
    "    diff = torch.abs(mixed_mag - target_image_mag)\n",
    "    \n",
    "    if mixed_mag.dim() == 3:\n",
    "        return diff.mean(dim=(1, 2))  # (B,)\n",
    "    return diff.mean()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian Smoothing for Musicality\n",
    "\n",
    "The mask is smoothed using a Gaussian kernel to enforce **temporal and spectral continuity**.\n",
    "This prevents harsh transitions that would create unmusical artifacts.\n",
    "\n",
    "$$\\text{mask}_{smooth} = \\text{mask} * G_\\sigma$$\n",
    "\n",
    "where $G_\\sigma$ is a 2D Gaussian kernel with standard deviation $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def gaussian_blur_2d(x, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Apply separable Gaussian blur to enforce smooth masks.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor (B, H, W) or (H, W)\n",
    "        sigma: Standard deviation of Gaussian kernel\n",
    "    \n",
    "    Returns:\n",
    "        Blurred tensor of same shape\n",
    "    \"\"\"\n",
    "    if sigma < 0.5:\n",
    "        return x\n",
    "    \n",
    "    # Kernel size: 6œÉ ensures we capture 99.7% of the Gaussian\n",
    "    kernel_size = int(6 * sigma) | 1  # Ensure odd\n",
    "    kernel_size = max(3, kernel_size)\n",
    "    \n",
    "    # Create 1D Gaussian kernel\n",
    "    coords = torch.arange(kernel_size, dtype=x.dtype, device=x.device) - kernel_size // 2\n",
    "    g = torch.exp(-coords**2 / (2 * sigma**2))\n",
    "    g = g / g.sum()  # Normalize\n",
    "    \n",
    "    # Ensure 4D for conv2d\n",
    "    pad = kernel_size // 2\n",
    "    original_dim = x.dim()\n",
    "    \n",
    "    if x.dim() == 2:\n",
    "        x = x.unsqueeze(0).unsqueeze(0)\n",
    "    elif x.dim() == 3:\n",
    "        x = x.unsqueeze(1)\n",
    "    \n",
    "    # Separable convolution (faster than 2D)\n",
    "    x = F.conv2d(F.pad(x, (pad, pad, 0, 0), mode='replicate'), \n",
    "                 g.view(1, 1, 1, -1), padding=0)\n",
    "    x = F.conv2d(F.pad(x, (0, 0, pad, pad), mode='replicate'), \n",
    "                 g.view(1, 1, -1, 1), padding=0)\n",
    "    \n",
    "    # Restore original dimensions\n",
    "    if original_dim == 2:\n",
    "        return x.squeeze(0).squeeze(0)\n",
    "    elif original_dim == 3:\n",
    "        return x.squeeze(1)\n",
    "    return x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Mask Encoder\n",
    "\n",
    "The encoder converts a **low-resolution control grid** (64√ó128) into:\n",
    "1. A **blended spectrogram** (513√óT)\n",
    "2. Optionally, **reconstructed audio** via inverse STFT\n",
    "\n",
    "### Blending Formula\n",
    "\n",
    "$$M_{mixed} = \\text{mask} \\cdot M_{image} + (1 - \\text{mask}) \\cdot M_{audio}$$\n",
    "\n",
    "where:\n",
    "- $\\text{mask} = 1$: Show the image\n",
    "- $\\text{mask} = 0$: Keep the audio\n",
    "\n",
    "### Proxy Optimization\n",
    "\n",
    "For speed, we optimize at **1/4 resolution** (129 frequency bins instead of 513).\n",
    "This provides a **4√ó speedup** with minimal quality loss."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# MaskProcessor - EXACT COPY from src/audio_encoder.py\n",
    "# =============================================================================\n",
    "class MaskProcessor(nn.Module):\n",
    "    \"\"\"Helper module for Mask generation to separate convolution logic.\"\"\"\n",
    "\n",
    "    def __init__(self, h, w, sigma, device):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.device = device\n",
    "        self._init_gaussian_kernel(sigma)\n",
    "\n",
    "    def _init_gaussian_kernel(self, sigma):\n",
    "        kernel_size = int(2 * math.ceil(2 * sigma) + 1)\n",
    "        x_cord = torch.arange(kernel_size)\n",
    "        x_grid = x_cord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "        y_grid = x_grid.t()\n",
    "        xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "\n",
    "        mean = (kernel_size - 1) / 2.0\n",
    "        variance = sigma**2.0\n",
    "        gaussian_kernel = (1.0 / (2.0 * math.pi * variance)) * torch.exp(\n",
    "            -torch.sum((xy_grid - mean) ** 2.0, dim=-1) / (2 * variance)\n",
    "        )\n",
    "        gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n",
    "        self.blur_kernel = gaussian_kernel.view(1, 1, kernel_size, kernel_size).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.kernel_padding = kernel_size // 2\n",
    "\n",
    "    def forward(self, params, target_h, target_w):\n",
    "        B = params.shape[0]\n",
    "        grid = params.view(B, 1, self.h, self.w)\n",
    "\n",
    "        # Blur the Low-Res Grid instead of the High-Res Up-sampled Mask\n",
    "        grid_blurred = F.conv2d(grid, self.blur_kernel, padding=self.kernel_padding)\n",
    "\n",
    "        mask = F.interpolate(\n",
    "            grid_blurred,\n",
    "            size=(target_h, target_w),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        return mask.squeeze(1)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MaskEncoder - EXACT COPY from src/audio_encoder.py\n",
    "# =============================================================================\n",
    "class MaskEncoder(nn.Module):\n",
    "    \"\"\"Encodes parameters into audio via mask-based spectrogram blending.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_image: torch.Tensor,\n",
    "        target_audio_path: str,\n",
    "        grid_height: int = 128,\n",
    "        grid_width: int = 256,\n",
    "        smoothing_sigma: float = 1.0,\n",
    "        device: str = DEVICE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.grid_height = grid_height\n",
    "        self.grid_width = grid_width\n",
    "        self.n_params = grid_height * grid_width\n",
    "        self.smoothing_sigma = smoothing_sigma\n",
    "\n",
    "        # 1. Load and Process Target Audio\n",
    "        audio, sr = torchaudio.load(target_audio_path)\n",
    "        if sr != SAMPLE_RATE:\n",
    "            audio = torchaudio.functional.resample(audio, sr, SAMPLE_RATE)\n",
    "\n",
    "        audio = audio.mean(dim=0, keepdim=True)  # Mix to mono\n",
    "\n",
    "        self.mask_processor = MaskProcessor(\n",
    "            grid_height, grid_width, smoothing_sigma, device\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"target_audio_waveform\", audio)\n",
    "\n",
    "        # Compute STFT\n",
    "        window = torch.hann_window(N_FFT).to(device)\n",
    "        stft = torch.stft(\n",
    "            audio.to(device),\n",
    "            n_fft=N_FFT,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            win_length=WIN_LENGTH,\n",
    "            window=window,\n",
    "            return_complex=True,\n",
    "        )\n",
    "\n",
    "        self.audio_mag = stft.abs() + 1e-8\n",
    "        self.audio_phase = stft.angle()\n",
    "\n",
    "        self.full_height = self.audio_mag.shape[1]\n",
    "        self.full_width = self.audio_mag.shape[2]\n",
    "\n",
    "        self.audio_log = torch.log(self.audio_mag)\n",
    "\n",
    "        # 2. Process Target Image\n",
    "        img = target_image.to(device)\n",
    "        if img.dim() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        target_h_visual = self.full_height\n",
    "        # FLIP image vertically: spectrograms have low freq at bottom, images have origin at top\n",
    "        img_flipped = torch.flip(img, dims=[-2])\n",
    "        img_full_freq = F.interpolate(\n",
    "            img_flipped,\n",
    "            size=(target_h_visual, self.full_width),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        img_resized = img_full_freq\n",
    "\n",
    "        # Dynamic Gain Staging\n",
    "        audio_log_max = torch.quantile(self.audio_log, 0.98)\n",
    "        audio_max_val = self.audio_log.max()\n",
    "        audio_floor_val = torch.quantile(self.audio_log, 0.01)\n",
    "\n",
    "        target_ceiling = audio_max_val - 0.5\n",
    "        headroom_nat = (audio_log_max - target_ceiling).item()\n",
    "        dynamic_range_nat = (target_ceiling - audio_floor_val).item() + 0.5\n",
    "        dynamic_range_nat = max(4.0, min(dynamic_range_nat, 12.0))\n",
    "\n",
    "        print(\"Dynamic Gain Staging:\")\n",
    "        print(f\"  > Audio Max: {audio_max_val:.2f}, Floor (q01): {audio_floor_val:.2f}\")\n",
    "        print(f\"  > Target Ceiling: {target_ceiling:.2f}\")\n",
    "        print(f\"  > Adaptive Dynamic Range: {dynamic_range_nat:.2f}\")\n",
    "\n",
    "        audio_log_ceil = audio_log_max - headroom_nat\n",
    "        audio_log_floor = audio_log_ceil - dynamic_range_nat\n",
    "\n",
    "        self.audio_log = torch.clamp(self.audio_log, min=audio_log_floor)\n",
    "\n",
    "        img_01 = img_resized.squeeze(0)\n",
    "        img_01 = (img_01 - img_01.min()) / (img_01.max() - img_01.min() + 1e-8)\n",
    "        img_01 = img_01.pow(1.8)  # Gamma correction\n",
    "\n",
    "        self.image_log = img_01 * (audio_log_ceil - audio_log_floor) + audio_log_floor\n",
    "        self.image_mag = torch.exp(self.image_log)\n",
    "        self.audio_mag_static = torch.exp(self.audio_log)\n",
    "\n",
    "        # PROXY OPTIMIZATION SETUP\n",
    "        self.proxy_height = 129\n",
    "        self.proxy_width = self.full_width // 2\n",
    "\n",
    "        self.image_mag_proxy = F.interpolate(\n",
    "            self.image_mag.unsqueeze(0),\n",
    "            size=(self.proxy_height, self.proxy_width),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).squeeze(0)\n",
    "\n",
    "        self.audio_mag_proxy = F.interpolate(\n",
    "            self.audio_mag_static.unsqueeze(0),\n",
    "            size=(self.proxy_height, self.proxy_width),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).squeeze(0)\n",
    "\n",
    "        # Expose PROXY as the default reference for Optimizers\n",
    "        self.image_mag_ref = self.image_mag_proxy\n",
    "        self.audio_mag = self.audio_mag_proxy\n",
    "\n",
    "        # Keep FULL res for final Export/Reconstruction\n",
    "        self.image_mag_full = self.image_mag\n",
    "        self.audio_mag_full = self.audio_mag_static\n",
    "\n",
    "        print(f\"Spectrogram: {self.full_height}√ó{self.full_width}\")\n",
    "        print(f\"Proxy: {self.proxy_height}√ó{self.proxy_width}\")\n",
    "\n",
    "    def _compute_spectrogram(self, mask, img_mag, aud_mag):\n",
    "        return mask * img_mag + (1 - mask) * aud_mag\n",
    "\n",
    "    def _reconstruct_audio(self, mixed_mag, phase):\n",
    "        complex_stft = torch.polar(mixed_mag, phase)\n",
    "\n",
    "        audio_recon = torch.istft(\n",
    "            complex_stft,\n",
    "            n_fft=N_FFT,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            win_length=WIN_LENGTH,\n",
    "            window=torch.hann_window(N_FFT, device=mixed_mag.device),\n",
    "        )\n",
    "\n",
    "        max_val = audio_recon.abs().max(dim=-1, keepdim=True)[0].clamp(min=1e-8)\n",
    "        audio_recon = audio_recon / max_val * 0.9\n",
    "        return audio_recon\n",
    "\n",
    "    def forward(self, params: torch.Tensor, return_wav: bool = True):\n",
    "        batch_size = params.shape[0]\n",
    "\n",
    "        # Generate Mask - target PROXY or FULL size based on mode\n",
    "        target_h = self.proxy_height if not return_wav else self.full_height\n",
    "        target_w = self.proxy_width if not return_wav else self.full_width\n",
    "\n",
    "        mask = self.mask_processor(params, target_h, target_w)\n",
    "\n",
    "        # Expand sources (Use PROXY or FULL based on mode)\n",
    "        if not return_wav:\n",
    "            img_mag = self.image_mag_ref.expand(batch_size, -1, -1)\n",
    "            aud_mag = self.audio_mag.expand(batch_size, -1, -1)\n",
    "        else:\n",
    "            img_mag = self.image_mag_full.expand(batch_size, -1, -1)\n",
    "            aud_mag = self.audio_mag_full.expand(batch_size, -1, -1)\n",
    "\n",
    "        phase = self.audio_phase.expand(batch_size, -1, -1) if return_wav else None\n",
    "\n",
    "        mixed_mag = self._compute_spectrogram(mask, img_mag, aud_mag)\n",
    "\n",
    "        audio_recon = None\n",
    "        if return_wav:\n",
    "            audio_recon = self._reconstruct_audio(mixed_mag, phase)\n",
    "\n",
    "        return audio_recon, mixed_mag"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data: Mona Lisa + Tchaikovsky"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load target image (matches backend preprocess_image)\n",
    "img_path = 'data/input/monalisa.jpg'\n",
    "img_pil = Image.open(img_path).convert('L')  # Grayscale\n",
    "img_tensor = torch.from_numpy(np.array(img_pil)).float() / 255.0\n",
    "img_tensor = img_tensor.unsqueeze(0)  # Add channel dim: (1, H, W)\n",
    "\n",
    "# Calculate grid dimensions (matches backend service.py lines 86-92)\n",
    "audio_path = 'data/input/tchaikovsky.mp3'\n",
    "waveform, sr = torchaudio.load(audio_path)\n",
    "duration_sec = waveform.shape[-1] / sr\n",
    "raw_width = int(duration_sec * 4.0)\n",
    "grid_width = ((raw_width + 15) // 16) * 16\n",
    "if grid_width < 16:\n",
    "    grid_width = 16\n",
    "grid_height = 64  # Matches backend\n",
    "\n",
    "# Create encoder with EXACT backend parameters\n",
    "sigma = 5.0  # Matches backend\n",
    "encoder = MaskEncoder(\n",
    "    img_tensor, \n",
    "    audio_path,\n",
    "    grid_height=grid_height,\n",
    "    grid_width=grid_width,\n",
    "    smoothing_sigma=sigma,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Update notebook constants to match\n",
    "GRID_HEIGHT = grid_height\n",
    "GRID_WIDTH = grid_width\n",
    "N_PARAMS = GRID_HEIGHT * GRID_WIDTH\n",
    "\n",
    "print(f\"\\nGrid: {GRID_HEIGHT}√ó{GRID_WIDTH} = {N_PARAMS} parameters\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize inputs\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(img_pil, cmap='gray')\n",
    "axes[0].set_title('Target Image: Mona Lisa', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Image as spectrogram (with frontend-matching scaling)\n",
    "plot_spectrogram(axes[1], encoder.image_mag_full[0], 'Image ‚Üí Spectrogram')\n",
    "axes[1].set_xlabel('Time (frames)')\n",
    "axes[1].set_ylabel('Frequency (bins)')\n",
    "\n",
    "# Target audio spectrogram\n",
    "plot_spectrogram(axes[2], encoder.audio_mag_full[0], 'Target Audio: Tchaikovsky')\n",
    "axes[2].set_xlabel('Time (frames)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Play original audio\n",
    "print(\"üîä Original Tchaikovsky:\")\n",
    "display(Audio(encoder.target_audio_waveform[0].numpy(), rate=SAMPLE_RATE))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Pareto Manager\n",
    "\n",
    "This class manages a **population of masks** for multi-objective optimization.\n",
    "It implements the **scalarization** approach where each individual optimizes a different\n",
    "weighted combination of visual and audio objectives.\n",
    "\n",
    "### Weight Distribution\n",
    "\n",
    "To counteract the 20√ó visual boost, we use a **power-law weight distribution**:\n",
    "\n",
    "$$w_{visual}^{(i)} = \\left(1 - \\frac{i}{N-1}\\right)^4$$\n",
    "\n",
    "This bunches more weights towards low visual priority, ensuring balanced coverage of the Pareto front.\n",
    "\n",
    "### Normalization\n",
    "\n",
    "Both losses are normalized by their worst-case values:\n",
    "\n",
    "$$\\hat{\\mathcal{L}}_{visual} = \\frac{\\mathcal{L}_{visual}}{\\max(\\mathcal{L}_{visual})}$$\n",
    "$$\\hat{\\mathcal{L}}_{audio} = \\frac{\\mathcal{L}_{audio}}{\\max(\\mathcal{L}_{audio})}$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class ParetoManager(nn.Module):\n",
    "    \"\"\"\n",
    "    Manages a population of masks for Pareto optimization.\n",
    "    \n",
    "    Uses scalarization with power-law weight distribution to find diverse\n",
    "    solutions along the Pareto front.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, pop_size=10, learning_rate=0.05):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.pop_size = pop_size\n",
    "        self.grid_h = encoder.grid_height\n",
    "        self.grid_w = encoder.grid_width\n",
    "        \n",
    "        # Initialize population in logit space (unbounded, sigmoid to [0,1])\n",
    "        self.mask_logits = nn.Parameter(\n",
    "            torch.randn(pop_size, self.grid_h * self.grid_w) * 0.5\n",
    "        )\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Power-law weight distribution (counteracts 20x visual boost)\n",
    "        # ====================================================================\n",
    "        alpha = torch.linspace(0, 1, pop_size)\n",
    "        \n",
    "        # weights = (1 - alpha)^4\n",
    "        # This bunches values near 0, so more individuals prioritize audio\n",
    "        # which balances the 20x visual boost during optimization\n",
    "        self.weights_img = (1.0 - alpha).pow(4.0)\n",
    "        self.weights_aud = 1.0 - self.weights_img\n",
    "        \n",
    "        # Normalize to sum to 1\n",
    "        total = self.weights_img + self.weights_aud\n",
    "        self.weights_img = self.weights_img / total\n",
    "        self.weights_aud = self.weights_aud / total\n",
    "        \n",
    "        # Force anchors: pure image and pure audio at extremes\n",
    "        if pop_size >= 2:\n",
    "            with torch.no_grad():\n",
    "                self.mask_logits[0].fill_(10.0)   # Index 0: Pure image\n",
    "                self.mask_logits[-1].fill_(-10.0)  # Index -1: Pure audio\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam([self.mask_logits], lr=learning_rate)\n",
    "        \n",
    "        # Normalization factors (will be computed)\n",
    "        self.scale_vis = 1.0\n",
    "        self.scale_aud = 1.0\n",
    "        \n",
    "    def calculate_normalization(self):\n",
    "        \"\"\"\n",
    "        Calculate worst-case losses to normalize objectives to [0, 1].\n",
    "        \"\"\"\n",
    "        print(\"Calculating normalization factors...\")\n",
    "        with torch.no_grad():\n",
    "            # Worst visual: pure audio (mask = 0)\n",
    "            max_vis_loss = torch.abs(\n",
    "                self.encoder.audio_mag - self.encoder.image_mag_ref\n",
    "            ).mean().item()\n",
    "            \n",
    "            # Worst audio: pure image (mask = 1)\n",
    "            max_aud_loss = calc_audio_mag_loss(\n",
    "                self.encoder.image_mag_ref, \n",
    "                self.encoder.audio_mag\n",
    "            ).item()\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            self.scale_vis = 1.0 / max(max_vis_loss, 1e-6)\n",
    "            self.scale_aud = 1.0 / max(max_aud_loss, 1e-6)\n",
    "            \n",
    "            print(f\"  Max visual loss: {max_vis_loss:.4f} ‚Üí scale: {self.scale_vis:.4f}\")\n",
    "            print(f\"  Max audio loss:  {max_aud_loss:.4f} ‚Üí scale: {self.scale_aud:.4f}\")\n",
    "    \n",
    "    def optimize_step(self):\n",
    "        \"\"\"\n",
    "        Perform one gradient descent step for all population members.\n",
    "        \n",
    "        MATCHES BACKEND: encoder(return_wav=False) automatically uses proxy.\n",
    "        \n",
    "        Returns:\n",
    "            avg_vis: Average normalized visual loss\n",
    "            avg_aud: Average normalized audio loss\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Get current masks\n",
    "        masks = torch.sigmoid(self.mask_logits)\n",
    "        \n",
    "        # Forward pass - return_wav=False uses PROXY automatically!\n",
    "        _, mixed_mag = self.encoder(masks, return_wav=False)\n",
    "        \n",
    "        # Calculate losses (matches backend - uses proxy refs)\n",
    "        diff = torch.abs(mixed_mag - self.encoder.image_mag_ref)\n",
    "        raw_loss_vis = diff.mean(dim=(1, 2))\n",
    "        raw_loss_aud = calc_audio_mag_loss(mixed_mag, self.encoder.audio_mag)\n",
    "        \n",
    "        # Normalize\n",
    "        loss_vis = raw_loss_vis * self.scale_vis\n",
    "        loss_aud = raw_loss_aud * self.scale_aud\n",
    "        \n",
    "        # Scalarized loss with 20x visual boost\n",
    "        total_loss = torch.sum(\n",
    "            self.weights_img * loss_vis * 20.0 + self.weights_aud * loss_aud\n",
    "        )\n",
    "        \n",
    "        # Backward and step\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Force clamp anchors\n",
    "        if self.pop_size >= 2:\n",
    "            with torch.no_grad():\n",
    "                self.mask_logits[0].fill_(20.0)\n",
    "                self.mask_logits[-1].fill_(-20.0)\n",
    "        \n",
    "        return loss_vis.detach().mean().item(), loss_aud.detach().mean().item()\n",
    "    \n",
    "    def get_current_front(self):\n",
    "        \"\"\"Get current population losses for plotting.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            masks = torch.sigmoid(self.mask_logits)\n",
    "            _, mixed_mag = self.encoder(masks, return_wav=False)\n",
    "            \n",
    "            diff = torch.abs(mixed_mag - self.encoder.image_mag_ref)\n",
    "            loss_vis = diff.mean(dim=(1, 2)) * self.scale_vis\n",
    "            loss_aud = calc_audio_mag_loss(mixed_mag, self.encoder.audio_mag) * self.scale_aud\n",
    "            \n",
    "            return loss_vis.numpy(), loss_aud.numpy()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Single-Objective Optimization\n",
    "\n",
    "This matches the frontend exactly when you adjust the \"Better Sound\" / \"Better Image\" sliders.\n",
    "\n",
    "The weights are passed directly as `(w_visual, w_audio)` and normalized to sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def single_objective_optimize(encoder, w_vis=0.5, w_aud=0.5, steps=SINGLE_STEPS, lr=0.05):\n",
    "    \"\"\"\n",
    "    Single-objective optimization with specified weights.\n",
    "    \n",
    "    This matches the frontend EXACTLY for the three presets:\n",
    "    - \"Better Image\": w_vis=0.8, w_aud=0.2\n",
    "    - \"Balanced\":     w_vis=0.5, w_aud=0.5\n",
    "    - \"Better Sound\": w_vis=0.2, w_aud=0.8\n",
    "    \n",
    "    Args:\n",
    "        encoder: MaskEncoder instance\n",
    "        w_vis: Visual weight (0-1)\n",
    "        w_aud: Audio weight (0-1)\n",
    "        steps: Number of optimization steps (default: 500)\n",
    "        lr: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "        mask: Optimized mask tensor\n",
    "        history: List of (vis_loss, aud_loss) tuples\n",
    "    \"\"\"\n",
    "    # Create single-individual manager\n",
    "    manager = ParetoManager(encoder, pop_size=1, learning_rate=lr)\n",
    "    manager.calculate_normalization()\n",
    "    \n",
    "    # Override weights (normalize to sum to 1)\n",
    "    total_weight = w_vis + w_aud\n",
    "    w_vis_norm = w_vis / total_weight\n",
    "    w_aud_norm = w_aud / total_weight\n",
    "    \n",
    "    manager.weights_img = torch.tensor([w_vis_norm])\n",
    "    manager.weights_aud = torch.tensor([w_aud_norm])\n",
    "    \n",
    "    # Initialize mask to neutral (logit = 0 ‚Üí sigmoid = 0.5)\n",
    "    with torch.no_grad():\n",
    "        manager.mask_logits.data.zero_()\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    print(f\"Optimizing with weights: Visual={w_vis_norm:.2f}, Audio={w_aud_norm:.2f}\")\n",
    "    print(f\"Steps: {steps}, Learning rate: {lr}\")\n",
    "    \n",
    "    for step in range(1, steps + 1):\n",
    "        avg_vis, avg_aud = manager.optimize_step()\n",
    "        history.append((avg_vis, avg_aud))\n",
    "        \n",
    "        if step % 100 == 0 or step == 1:\n",
    "            print(f\"  Step {step:4d}: Visual={avg_vis:.4f}, Audio={avg_aud:.4f}\")\n",
    "    \n",
    "    # Get final mask\n",
    "    final_mask = torch.sigmoid(manager.mask_logits).detach()\n",
    "    \n",
    "    return final_mask, history"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run the three frontend presets\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üé® VISUAL PRIORITY (80/20) - matches 'Better Image' button\")\n",
    "print(\"=\" * 60)\n",
    "start = time.time()\n",
    "mask_visual, hist_visual = single_objective_optimize(encoder, w_vis=0.8, w_aud=0.2)\n",
    "print(f\"Completed in {time.time() - start:.1f}s\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚öñÔ∏è  BALANCED (50/50)\")\n",
    "print(\"=\" * 60)\n",
    "start = time.time()\n",
    "mask_balanced, hist_balanced = single_objective_optimize(encoder, w_vis=0.5, w_aud=0.5)\n",
    "print(f\"Completed in {time.time() - start:.1f}s\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîä AUDIO PRIORITY (20/80) - matches 'Better Sound' button\")\n",
    "print(\"=\" * 60)\n",
    "start = time.time()\n",
    "mask_audio, hist_audio = single_objective_optimize(encoder, w_vis=0.2, w_aud=0.8)\n",
    "print(f\"Completed in {time.time() - start:.1f}s\\n\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot convergence\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, hist, title, color in [\n",
    "    (axes[0], hist_visual, 'Visual Priority (80/20)', 'purple'),\n",
    "    (axes[1], hist_balanced, 'Balanced (50/50)', 'gray'),\n",
    "    (axes[2], hist_audio, 'Audio Priority (20/80)', 'green'),\n",
    "]:\n",
    "    vis_losses = [h[0] for h in hist]\n",
    "    aud_losses = [h[1] for h in hist]\n",
    "    \n",
    "    ax.plot(vis_losses, label='Visual Loss', color='purple', alpha=0.8)\n",
    "    ax.plot(aud_losses, label='Audio Loss', color='green', alpha=0.8)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Normalized Loss')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "results = [\n",
    "    (mask_visual, 'Visual Priority (80/20)'),\n",
    "    (mask_balanced, 'Balanced (50/50)'),\n",
    "    (mask_audio, 'Audio Priority (20/80)')\n",
    "]\n",
    "\n",
    "for i, (mask, title) in enumerate(results):\n",
    "    # Generate full-resolution output\n",
    "    _, mixed_mag = encoder(mask, return_wav=True)  # return_wav=True = full resolution\n",
    "    \n",
    "    # Mask visualization\n",
    "    axes[0, i].imshow(\n",
    "        mask.view(GRID_HEIGHT, GRID_WIDTH).numpy(), \n",
    "        cmap='gray', vmin=0, vmax=1\n",
    "    )\n",
    "    axes[0, i].set_title(f'{title}\\nMask (white=image, black=audio)')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Spectrogram (with frontend-matching scaling)\n",
    "    plot_spectrogram(axes[1, i], mixed_mag[0], 'Result Spectrogram')\n",
    "    axes[1, i].set_xlabel('Time')\n",
    "    axes[1, i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Listen to results\n",
    "print(\"üîä AUDIO PRIORITY (20/80) - Best sound quality:\")\n",
    "wav_audio, _ = encoder(mask_audio, return_wav=True)\n",
    "display(Audio(wav_audio[0].numpy(), rate=SAMPLE_RATE))\n",
    "\n",
    "print(\"\\n‚öñÔ∏è  BALANCED (50/50):\")\n",
    "wav_balanced, _ = encoder(mask_balanced, return_wav=True)\n",
    "display(Audio(wav_balanced[0].numpy(), rate=SAMPLE_RATE))\n",
    "\n",
    "print(\"\\nüé® VISUAL PRIORITY (80/20) - Best visual fidelity:\")\n",
    "wav_visual, _ = encoder(mask_visual, return_wav=True)\n",
    "display(Audio(wav_visual[0].numpy(), rate=SAMPLE_RATE))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Pareto Frontier\n",
    "\n",
    "This matches the \"Map Full Pareto Frontier\" button in the frontend EXACTLY.\n",
    "\n",
    "### Hybrid Optimization Strategy\n",
    "\n",
    "1. **Phase 1: Gradient Seeding** (200 steps, population=10)\n",
    "   - Run parallel Adam optimization with power-law weight distribution\n",
    "   - Force anchor solutions at extremes (pure image, pure audio)\n",
    "   - Fast exploration of the objective space\n",
    "\n",
    "2. **Phase 2: Evolutionary Expansion** (50 generations, population=100)\n",
    "   - NSGA-II with SBX crossover and polynomial mutation\n",
    "   - Refines and expands the Pareto front\n",
    "   - Non-dominated sorting ensures quality"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def run_pareto_optimization(encoder, \n",
    "                            seed_steps=PARETO_SEED_STEPS,\n",
    "                            seed_pop=PARETO_SEED_POP,\n",
    "                            n_gen=PARETO_GENERATIONS,\n",
    "                            evol_pop=PARETO_EVOL_POP):\n",
    "    \"\"\"\n",
    "    Full Pareto frontier optimization matching the frontend exactly.\n",
    "    \n",
    "    Args:\n",
    "        encoder: MaskEncoder instance\n",
    "        seed_steps: Gradient seeding steps (default: 200)\n",
    "        seed_pop: Seeding population size (default: 10)\n",
    "        n_gen: NSGA-II generations (default: 50)\n",
    "        evol_pop: Evolution population size (default: 100)\n",
    "    \n",
    "    Returns:\n",
    "        final_X: (N, params) Pareto-optimal mask parameters\n",
    "        final_F: (N, 2) Pareto-optimal objective values\n",
    "        full_history: List of population snapshots for animation\n",
    "    \"\"\"\n",
    "    from pymoo.core.problem import Problem\n",
    "    from pymoo.core.callback import Callback\n",
    "    from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "    from pymoo.optimize import minimize\n",
    "    from pymoo.operators.crossover.sbx import SBX\n",
    "    from pymoo.operators.mutation.pm import PM\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Phase 1: Gradient Seeding\n",
    "    # ========================================================================\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PHASE 1: GRADIENT SEEDING\")\n",
    "    print(f\"Population: {seed_pop}, Steps: {seed_steps}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    manager = ParetoManager(encoder, pop_size=seed_pop, learning_rate=0.05)\n",
    "    manager.calculate_normalization()\n",
    "    \n",
    "    phase1_history = []\n",
    "    \n",
    "    for step in range(1, seed_steps + 1):\n",
    "        manager.optimize_step()\n",
    "        \n",
    "        # Record history every 5 steps (for animation)\n",
    "        if step % 5 == 0:\n",
    "            vis, aud = manager.get_current_front()\n",
    "            phase1_history.append(np.column_stack([vis, aud]))\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            vis, aud = manager.get_current_front()\n",
    "            print(f\"  Step {step:4d}: Vis range [{vis.min():.3f}, {vis.max():.3f}], \"\n",
    "                  f\"Aud range [{aud.min():.3f}, {aud.max():.3f}]\")\n",
    "    \n",
    "    # Extract seeds\n",
    "    with torch.no_grad():\n",
    "        seed_masks = torch.sigmoid(manager.mask_logits).cpu().numpy()\n",
    "    \n",
    "    print(f\"\\nGenerated {len(seed_masks)} seed solutions\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Phase 2: Evolutionary Expansion\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PHASE 2: EVOLUTIONARY EXPANSION (NSGA-II)\")\n",
    "    print(f\"Population: {evol_pop}, Generations: {n_gen}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    class MOSSProblem(Problem):\n",
    "        def __init__(self, enc, scale_vis, scale_aud):\n",
    "            self.enc = enc\n",
    "            self.scale_vis = scale_vis\n",
    "            self.scale_aud = scale_aud\n",
    "            super().__init__(\n",
    "                n_var=enc.grid_height * enc.grid_width,\n",
    "                n_obj=2,\n",
    "                xl=0.0,\n",
    "                xu=1.0\n",
    "            )\n",
    "        \n",
    "        def _evaluate(self, x, out, *args, **kwargs):\n",
    "            with torch.no_grad():\n",
    "                masks = torch.from_numpy(x).float()\n",
    "                _, mixed_mag = self.enc(masks, return_wav=False)  # proxy automatically\n",
    "                \n",
    "                vis = calc_visual_loss(mixed_mag, self.enc.image_mag_ref).numpy()\n",
    "                aud = calc_audio_mag_loss(mixed_mag, self.enc.audio_mag).numpy()\n",
    "                \n",
    "                vis = vis * self.scale_vis\n",
    "                aud = aud * self.scale_aud\n",
    "                \n",
    "                out['F'] = np.column_stack([vis, aud])\n",
    "    \n",
    "    class HistoryCallback(Callback):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.history = []\n",
    "        \n",
    "        def notify(self, algorithm):\n",
    "            # Record surviving population (not just offspring)\n",
    "            self.history.append(algorithm.pop.get('F').copy())\n",
    "    \n",
    "    problem = MOSSProblem(encoder, manager.scale_vis, manager.scale_aud)\n",
    "    \n",
    "    # Initialize with seeds + random\n",
    "    n_random = evol_pop - len(seed_masks)\n",
    "    X_init = np.vstack([\n",
    "        seed_masks,\n",
    "        np.random.rand(n_random, problem.n_var)\n",
    "    ])\n",
    "    \n",
    "    algorithm = NSGA2(\n",
    "        pop_size=evol_pop,\n",
    "        sampling=X_init,\n",
    "        crossover=SBX(eta=15, prob=0.9),\n",
    "        mutation=PM(eta=20),\n",
    "        eliminate_duplicates=True\n",
    "    )\n",
    "    \n",
    "    callback = HistoryCallback()\n",
    "    \n",
    "    result = minimize(\n",
    "        problem,\n",
    "        algorithm,\n",
    "        ('n_gen', n_gen),\n",
    "        callback=callback,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    full_history = phase1_history + callback.history\n",
    "    \n",
    "    print(f\"\\n‚úÖ Found {len(result.F)} Pareto-optimal solutions\")\n",
    "    \n",
    "    return result.X, result.F, full_history"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run full Pareto optimization\n",
    "start = time.time()\n",
    "pareto_X, pareto_F, pareto_history = run_pareto_optimization(encoder)\n",
    "print(f\"\\nTotal time: {time.time() - start:.1f}s\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize evolution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Final Pareto front\n",
    "axes[0].scatter(\n",
    "    pareto_F[:, 0], pareto_F[:, 1],\n",
    "    c='#4ade80', s=60, edgecolors='white', linewidths=1,\n",
    "    label='Final Pareto Front', zorder=10\n",
    ")\n",
    "\n",
    "# Seeds for reference\n",
    "if pareto_history:\n",
    "    seeds = pareto_history[len(pareto_history) // 4]  # Early seeding\n",
    "    axes[0].scatter(\n",
    "        seeds[:, 0], seeds[:, 1],\n",
    "        c='purple', s=40, alpha=0.3, label='Seeding Phase'\n",
    "    )\n",
    "\n",
    "axes[0].set_xlabel('Visual Loss (normalized) ‚Üì')\n",
    "axes[0].set_ylabel('Audio Loss (normalized) ‚Üì')\n",
    "axes[0].set_title('Pareto Frontier: Visual vs Audio Quality')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Evolution over time\n",
    "n_hist = len(pareto_history)\n",
    "phase1_len = PARETO_SEED_STEPS // 5  # History recorded every 5 steps\n",
    "\n",
    "for i, frame in enumerate(pareto_history):\n",
    "    if i < phase1_len:\n",
    "        color = 'purple'\n",
    "        alpha = 0.1 + 0.3 * (i / phase1_len)\n",
    "    else:\n",
    "        color = '#4ade80'\n",
    "        alpha = 0.2 + 0.6 * ((i - phase1_len) / (n_hist - phase1_len))\n",
    "    \n",
    "    axes[1].scatter(frame[:, 0], frame[:, 1], c=color, alpha=alpha, s=20)\n",
    "\n",
    "axes[1].set_xlabel('Visual Loss (normalized) ‚Üì')\n",
    "axes[1].set_ylabel('Audio Loss (normalized) ‚Üì')\n",
    "axes[1].set_title('Evolution Over Time (purple=seeding, green=evolution)')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated Evolution (Matches Frontend)\n",
    "\n",
    "This animation shows the optimization process with:\n",
    "- **Phase 1 (Purple)**: Gradient seeding with smooth point transitions\n",
    "- **Phase 2 (Green)**: NSGA-II evolution with fade trails"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def create_pareto_animation(history, phase1_frames=40):\n",
    "    \"\"\"\n",
    "    Create an animated visualization of Pareto optimization.\n",
    "    Matches frontend/backend animation style (light mode).\n",
    "    \"\"\"\n",
    "    if not history or len(history) == 0:\n",
    "        print(\"No history to animate\")\n",
    "        return None\n",
    "    \n",
    "    # Setup Figure (light mode version)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.set_title(\"Pareto Optimization Animation\", fontsize=14, pad=10)\n",
    "    ax.set_xlabel(\"Visual Loss (normalized) ‚Üì\")\n",
    "    ax.set_ylabel(\"Audio Loss (normalized) ‚Üì\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Find global bounds\n",
    "    all_points = np.vstack(history)\n",
    "    min_x, max_x = all_points[:, 0].min(), all_points[:, 0].max()\n",
    "    min_y, max_y = all_points[:, 1].min(), all_points[:, 1].max()\n",
    "    \n",
    "    pad_x = (max_x - min_x) * 0.1 if max_x > min_x else 0.1\n",
    "    pad_y = (max_y - min_y) * 0.1 if max_y > min_y else 0.1\n",
    "    \n",
    "    ax.set_xlim(min_x - pad_x, max_x + pad_x)\n",
    "    ax.set_ylim(min_y - pad_y, max_y + pad_y)\n",
    "    \n",
    "    # Scatter for current frame\n",
    "    scat = ax.scatter([], [], c='#a855f7', alpha=0.8, s=60, \n",
    "                      edgecolors='white', linewidths=0.5)\n",
    "    \n",
    "    # Trail scatters for fade effect\n",
    "    trail_depth = 5\n",
    "    trail_scats = [ax.scatter([], [], c='#4ade80', alpha=0, s=40) \n",
    "                   for _ in range(trail_depth)]\n",
    "    \n",
    "    text = ax.text(0.02, 0.98, '', transform=ax.transAxes, fontsize=10, va='top')\n",
    "    \n",
    "    prev_positions = [None]  # Use list to allow mutation in closure\n",
    "    \n",
    "    def match_points(prev, curr):\n",
    "        \"\"\"Match points between frames for smooth transition.\"\"\"\n",
    "        if prev is None or len(prev) != len(curr):\n",
    "            return curr\n",
    "        dist = cdist(prev, curr)\n",
    "        matched = np.zeros_like(curr)\n",
    "        used = set()\n",
    "        for i in range(len(prev)):\n",
    "            dists = dist[i].copy()\n",
    "            dists[list(used)] = np.inf\n",
    "            j = np.argmin(dists)\n",
    "            matched[i] = curr[j]\n",
    "            used.add(j)\n",
    "        return matched\n",
    "    \n",
    "    def update(frame):\n",
    "        if frame >= len(history):\n",
    "            return (scat, text, *trail_scats)\n",
    "        \n",
    "        data = history[frame]\n",
    "        \n",
    "        # Smooth transition during seeding phase\n",
    "        if frame < phase1_frames:\n",
    "            data = match_points(prev_positions[0], data)\n",
    "        prev_positions[0] = data.copy()\n",
    "        \n",
    "        scat.set_offsets(data)\n",
    "        \n",
    "        # Phase-based coloring\n",
    "        if frame < phase1_frames:\n",
    "            scat.set_facecolors('#a855f7')  # Purple for seeding\n",
    "            text.set_text(f'Phase 1: Gradient Seeding (Step {frame * 5})')\n",
    "            for ts in trail_scats:\n",
    "                ts.set_offsets(np.empty((0, 2)))\n",
    "        else:\n",
    "            scat.set_facecolors('#4ade80')  # Green for evolution\n",
    "            gen = frame - phase1_frames\n",
    "            text.set_text(f'Phase 2: Evolutionary (Gen {gen})')\n",
    "            \n",
    "            # Update trails with fade effect\n",
    "            for i, ts in enumerate(trail_scats):\n",
    "                trail_frame = frame - (i + 1)\n",
    "                if trail_frame >= phase1_frames and trail_frame < len(history):\n",
    "                    ts.set_offsets(history[trail_frame])\n",
    "                    alpha = 0.3 * (1 - (i / trail_depth))\n",
    "                    ts.set_alpha(alpha)\n",
    "                else:\n",
    "                    ts.set_offsets(np.empty((0, 2)))\n",
    "        \n",
    "        return (scat, text, *trail_scats)\n",
    "    \n",
    "    ani = FuncAnimation(fig, update, frames=len(history), interval=100, blit=True)\n",
    "    plt.close(fig)  # Prevent static display\n",
    "    \n",
    "    return ani\n",
    "\n",
    "# Create and display animation\n",
    "phase1_len = PARETO_SEED_STEPS // 5\n",
    "ani = create_pareto_animation(pareto_history, phase1_frames=phase1_len)\n",
    "if ani:\n",
    "    display(HTML(ani.to_jshtml()))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Explore the Pareto front\n",
    "sorted_idx = np.argsort(pareto_F[:, 0])\n",
    "\n",
    "# Sample 5 points along the front\n",
    "n_samples = min(5, len(sorted_idx))\n",
    "sample_indices = [sorted_idx[int(i * (len(sorted_idx) - 1) / (n_samples - 1))] \n",
    "                  for i in range(n_samples)]\n",
    "\n",
    "fig, axes = plt.subplots(2, n_samples, figsize=(4 * n_samples, 8))\n",
    "\n",
    "for col, idx in enumerate(sample_indices):\n",
    "    mask = torch.from_numpy(pareto_X[idx:idx+1]).float()\n",
    "    wav, mixed_mag = encoder(mask, return_wav=True)\n",
    "    \n",
    "    # Mask\n",
    "    axes[0, col].imshow(\n",
    "        mask.view(GRID_HEIGHT, GRID_WIDTH).numpy(),\n",
    "        cmap='gray', vmin=0, vmax=1\n",
    "    )\n",
    "    axes[0, col].set_title(f'V:{pareto_F[idx,0]:.3f}, A:{pareto_F[idx,1]:.3f}')\n",
    "    axes[0, col].axis('off')\n",
    "    \n",
    "    # Spectrogram (frontend-matching scaling)\n",
    "    plot_spectrogram(axes[1, col], mixed_mag[0])\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "plt.suptitle('Samples Across the Pareto Front (Left=Best Visual, Right=Best Audio)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Listen to samples\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    mask = torch.from_numpy(pareto_X[idx:idx+1]).float()\n",
    "    wav, _ = encoder(mask, return_wav=True)\n",
    "    print(f\"\\nüéµ Sample {i+1}: Visual={pareto_F[idx,0]:.3f}, Audio={pareto_F[idx,1]:.3f}\")\n",
    "    display(Audio(wav[0].numpy(), rate=SAMPLE_RATE))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Mask-based blending** allows smooth control between image and audio fidelity\n",
    "2. **Proxy optimization** (1/4 resolution) provides 4√ó speedup with minimal quality loss\n",
    "3. **20√ó visual boost** is necessary to make the image visible\n",
    "4. **Power-law weights** counteract the boost for balanced Pareto coverage\n",
    "5. **Hybrid optimization** (gradient + evolutionary) provides thorough exploration\n",
    "\n",
    "### Design Decisions\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|-----------|-------|-----------|\n",
    "| Sample rate | 16 kHz | Fast processing, sufficient for demos |\n",
    "| Grid size | 64√ó128 | Balance between control and optimization speed |\n",
    "| Smoothing œÉ | 5.0 | Prevents harsh artifacts, ensures musicality |\n",
    "| Visual boost | 20√ó | Compensates for audio loss dominance |\n",
    "| Weight power | 4 | Counteracts 20√ó boost for balanced coverage |\n",
    "\n",
    "---\n",
    "\n",
    "*Made with üíú by [Vojtƒõch Kucha≈ô](https://github.com/kuchar-one)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}